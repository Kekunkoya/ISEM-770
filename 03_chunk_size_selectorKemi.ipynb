{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Evaluating Chunk Sizes in Simple RAG\n",
    "\n",
    "Choosing the right chunk size is crucial for improving retrieval accuracy in a Retrieval-Augmented Generation (RAG) pipeline. The goal is to balance retrieval performance with response quality.\n",
    "\n",
    "This section evaluates different chunk sizes by:\n",
    "\n",
    "1. Extracting text from a PDF.\n",
    "2. Splitting text into chunks of varying sizes.\n",
    "3. Creating embeddings for each chunk.\n",
    "4. Retrieving relevant chunks for a query.\n",
    "5. Generating a response using retrieved chunks.\n",
    "6. Evaluating faithfulness and relevancy.\n",
    "7. Comparing results for different chunk sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from the PDF\n",
    "First, we will extract text from the `AI_Information.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past f\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(\"/Users/kekunkoya/Desktop/ISEM 770 Class Project/AI_Information.pdf\")\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page in mypdf:\n",
    "        # Extract text from the current page and add spacing\n",
    "        all_text += page.get_text(\"text\") + \" \"\n",
    "\n",
    "    # Return the extracted text, stripped of leading/trailing whitespace\n",
    "    return all_text.strip()\n",
    "\n",
    "# Define the path to the PDF file\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Print the first 500 characters of the extracted text\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "To improve retrieval, we split the extracted text into overlapping chunks of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 128, Number of Chunks: 326\n",
      "Chunk Size: 256, Number of Chunks: 164\n",
      "Chunk Size: 512, Number of Chunks: 82\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): Number of characters per chunk.\n",
    "    overlap (int): Overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from the current index to the index + chunk size\n",
    "        chunks.append(text[i:i + n])\n",
    "    \n",
    "    return chunks  # Return the list of text chunks\n",
    "\n",
    "# Define different chunk sizes to evaluate\n",
    "chunk_sizes = [128, 256, 512]\n",
    "\n",
    "# Create a dictionary to store text chunks for each chunk size\n",
    "text_chunks_dict = {size: chunk_text(extracted_text, size, size // 5) for size in chunk_sizes}\n",
    "\n",
    "# Print the number of chunks created for each chunk size\n",
    "for size, chunks in text_chunks_dict.items():\n",
    "    print(f\"Chunk Size: {size}, Number of Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings convert text into numerical representations for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts.\n",
    "\n",
    "    Args:\n",
    "    texts (List[str]): List of input texts.\n",
    "    model (str): Embedding model.\n",
    "\n",
    "    Returns:\n",
    "    List[np.ndarray]: List of numerical embeddings.\n",
    "    \"\"\"\n",
    "    # Create embeddings using the specified model\n",
    "    response = client.embeddings.create(model=model, input=texts)\n",
    "    # Convert the response to a list of numpy arrays and return\n",
    "    return [np.array(embedding.embedding) for embedding in response.data]\n",
    "\n",
    "# Generate embeddings for each chunk size\n",
    "# Iterate over each chunk size and its corresponding chunks in the text_chunks_dict\n",
    "chunk_embeddings_dict = {size: create_embeddings(chunks) for size, chunks in tqdm(text_chunks_dict.items(), desc=\"Generating Embeddings\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We use cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): First vector.\n",
    "    vec2 (np.ndarray): Second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the dot product of the two vectors\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, text_chunks, chunk_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant text chunks.\n",
    "    \n",
    "    Args:\n",
    "    query (str): User query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    chunk_embeddings (List[np.ndarray]): Embeddings of text chunks.\n",
    "    k (int): Number of top chunks to return.\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: Most relevant text chunks.\n",
    "    \"\"\"\n",
    "    # Generate an embedding for the query - pass query as a list and get first item\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # Calculate cosine similarity between the query embedding and each chunk embedding\n",
    "    similarities = [cosine_similarity(query_embedding, emb) for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get the indices of the top-k most similar chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k most relevant text chunks\n",
    "    return [text_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top chunks for chunk_size=256:\n",
      "['chunk1 for size 256', 'chunk2 for size 256']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1) Load your validation data ---\n",
    "val_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/val.json'\n",
    "if not os.path.isfile(val_path):\n",
    "    raise FileNotFoundError(f\"Could not find validation file at: {val_path!r}\")\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "query = data[3]['question']\n",
    "\n",
    "# --- 2) Define your chunk sizes and sample text/embeddings dicts ---\n",
    "#    Replace these with your actual chunks and embeddings.\n",
    "chunk_sizes = [128, 256, 512]\n",
    "text_chunks_dict = {\n",
    "    size: [\"chunk1 for size \"+str(size), \"chunk2 for size \"+str(size)]  # ← REPLACE\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "# here we simulate embeddings as random vectors; replace with your real embeddings\n",
    "chunk_embeddings_dict = {\n",
    "    size: np.random.rand(len(text_chunks_dict[size]), 768)\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "\n",
    "# --- 3) Define retrieval function inline ---\n",
    "def retrieve_relevant_chunks(query: str,\n",
    "                             chunks: list[str],\n",
    "                             embeddings: np.ndarray,\n",
    "                             top_k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Example using cosine similarity. Replace `embed_query`\n",
    "    with however you turn your query into a vector.\n",
    "    \"\"\"\n",
    "    # --- a) Embed the query (stubbed as random) ---\n",
    "    #    Replace this with your real query embedding call!\n",
    "    query_emb = np.random.rand(1, embeddings.shape[1])\n",
    "    \n",
    "    # --- b) Compute similarities and pick top_k ---\n",
    "    sims = cosine_similarity(query_emb, embeddings)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:top_k]\n",
    "    return [chunks[i] for i in top_idx]\n",
    "\n",
    "# --- 4) Run retrieval across sizes ---\n",
    "retrieved_chunks_dict = {\n",
    "    size: retrieve_relevant_chunks(\n",
    "        query,\n",
    "        text_chunks_dict[size],\n",
    "        chunk_embeddings_dict[size]\n",
    "    )\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "\n",
    "# --- 5) Print results for size=256 ---\n",
    "print(\"Top chunks for chunk_size=256:\")\n",
    "print(retrieved_chunks_dict.get(256, 'No chunks for 256'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks\n",
    "Let's  generate a response based on the retrieved text for chunk size `256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI response (chunk_size=256):\n",
      "I do not have enough information to answer that.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# 0) Initialize your OpenAI client\n",
    "#    Make sure OPENAI_API_KEY is set in your environment\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 1) Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant that strictly answers based on the given context. \"\n",
    "    \"If the answer cannot be derived directly from the provided context, \"\n",
    "    \"respond with: 'I do not have enough information to answer that.'\"\n",
    ")\n",
    "\n",
    "# 2) Define the response generator\n",
    "def generate_response(query, system_prompt, retrieved_chunks, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates an AI response based on retrieved chunks.\n",
    "    \"\"\"\n",
    "    # Combine retrieved chunks into a single context string\n",
    "    context = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" \n",
    "                            for i, chunk in enumerate(retrieved_chunks)])\n",
    "    \n",
    "    # Build the messages payload\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": f\"{context}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    # Call the chat completion endpoint\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 3) (Re)define your query and chunk_sizes if not already in scope\n",
    "#    query = data[3]['question']\n",
    "#    chunk_sizes = [128, 256, 512]\n",
    "#    retrieved_chunks_dict = {...}  # as built previously\n",
    "\n",
    "# 4) Generate responses for each chunk size\n",
    "ai_responses_dict = {\n",
    "    size: generate_response(query, system_prompt, retrieved_chunks_dict[size])\n",
    "    for size in chunk_sizes\n",
    "}\n",
    "\n",
    "# 5) Print the response for chunk size 256\n",
    "print(\"AI response (chunk_size=256):\")\n",
    "print(ai_responses_dict.get(256, \"No response for size 256\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We score responses based on faithfulness and relevancy using powerfull llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation scoring system constants\n",
    "SCORE_FULL = 1.0     # Complete match or fully satisfactory\n",
    "SCORE_PARTIAL = 0.5  # Partial match or somewhat satisfactory\n",
    "SCORE_NONE = 0.0     # No match or unsatisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strict evaluation prompt templates\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the faithfulness of the AI response compared to the true answer.\n",
    "User Query: {question}\n",
    "AI Response: {response}\n",
    "True Answer: {true_answer}\n",
    "\n",
    "Faithfulness measures how well the AI response aligns with facts in the true answer, without hallucinations.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {full} = Completely faithful, no contradictions with true answer\n",
    "    * {partial} = Partially faithful, minor contradictions\n",
    "    * {none} = Not faithful, major contradictions or hallucinations\n",
    "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the relevancy of the AI response to the user query.\n",
    "User Query: {question}\n",
    "AI Response: {response}\n",
    "\n",
    "Relevancy measures how well the response addresses the user's question.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {full} = Completely relevant, directly addresses the query\n",
    "    * {partial} = Partially relevant, addresses some aspects\n",
    "    * {none} = Not relevant, fails to address the query\n",
    "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness (256): 0.0, Relevancy (256): 0.0\n",
      "Faithfulness (128): 0.0, Relevancy (128): 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# 0) Initialize OpenAI client (make sure OPENAI_API_KEY is set)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 1) Load your validation data\n",
    "val_path = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/val.json'\n",
    "if not os.path.isfile(val_path):\n",
    "    raise FileNotFoundError(f\"Could not find {val_path!r}\")\n",
    "with open(val_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "query = data[3]['question']\n",
    "true_answer = data[3]['ideal_answer']\n",
    "\n",
    "# 2) Define your prompts & scoring constants\n",
    "SCORE_FULL    = \"1.0\"\n",
    "SCORE_PARTIAL = \"0.5\"\n",
    "SCORE_NONE    = \"0.0\"\n",
    "\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the FAITHFULNESS of the assistant’s response given the true answer.\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "True Answer: {true_answer}\n",
    "Return:\n",
    "- {full} if the response is fully supported by the true answer.\n",
    "- {partial} if it’s partially supported.\n",
    "- {none} if it’s unsupported.\n",
    "Just output ONLY the numeric score.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the RELEVANCY of the assistant’s response to the user’s question.\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "Return:\n",
    "- {full} if the response directly addresses the question.\n",
    "- {partial} if it somewhat addresses it.\n",
    "- {none} if it does not address it.\n",
    "Just output ONLY the numeric score.\n",
    "\"\"\"\n",
    "\n",
    "# 3) Define the evaluator\n",
    "def evaluate_response(question, response, true_answer):\n",
    "    # Build prompts\n",
    "    faith_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        response=response,\n",
    "        true_answer=true_answer,\n",
    "        full=SCORE_FULL,\n",
    "        partial=SCORE_PARTIAL,\n",
    "        none=SCORE_NONE\n",
    "    )\n",
    "    rel_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        response=response,\n",
    "        full=SCORE_FULL,\n",
    "        partial=SCORE_PARTIAL,\n",
    "        none=SCORE_NONE\n",
    "    )\n",
    "\n",
    "    # Ask the LLM (use a chat model, not an embedding model)\n",
    "    faith_resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numeric score.\"},\n",
    "            {\"role\": \"user\",   \"content\": faith_prompt}\n",
    "        ]\n",
    "    )\n",
    "    rel_resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numeric score.\"},\n",
    "            {\"role\": \"user\",   \"content\": rel_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Parse scores\n",
    "    try:\n",
    "        faith_score = float(faith_resp.choices[0].message.content.strip())\n",
    "    except ValueError:\n",
    "        faith_score = 0.0\n",
    "    try:\n",
    "        rel_score = float(rel_resp.choices[0].message.content.strip())\n",
    "    except ValueError:\n",
    "        rel_score = 0.0\n",
    "\n",
    "    return faith_score, rel_score\n",
    "\n",
    "# 4) Assume you've already generated `ai_responses_dict` and `chunk_sizes`\n",
    "#    e.g., ai_responses_dict = {256: \"...\", 128: \"...\", ...}\n",
    "\n",
    "# Evaluate for chunk sizes 256 and 128\n",
    "faith256, rel256 = evaluate_response(query, ai_responses_dict[256], true_answer)\n",
    "faith128, rel128 = evaluate_response(query, ai_responses_dict[128], true_answer)\n",
    "\n",
    "print(f\"Faithfulness (256): {faith256}, Relevancy (256): {rel256}\")\n",
    "print(f\"Faithfulness (128): {faith128}, Relevancy (128): {rel128}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISEM 770 Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
