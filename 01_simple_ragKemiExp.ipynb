{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction to Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\n",
    "\n",
    "In a Simple RAG setup, we follow these steps:\n",
    "\n",
    "1. **Data Ingestion**: Load and preprocess the text data.\n",
    "2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n",
    "3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieved text.\n",
    "\n",
    "This notebook implements a Simple RAG approach, evaluates the modelâ€™s response, and explores various improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    " \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 65\n",
      "\n",
      "First text chunk:\n",
      "19\n",
      "Defining and Measuring Homelessness\n",
      "Volker Busch-Geertsema\n",
      "GISS, Germany\n",
      ">>Abstract _ Substantial progress has been made at EU level on defining home -\n",
      "lessness. The European Typology on Homelessness and Housing Exclusion \n",
      "(ETHOS) is widely accepted in almost all European countries (and beyond) as \n",
      "a useful conceptual framework and almost everywhere definitions at national \n",
      "level (though often not identical with ETHOS) are discussed in relation to this typology. The development and some of the remaining controversial issues concerning ETHOS and a reduced version of it are discussed in this chapter. Furthermore essential reasons and different approaches to measure home -\n",
      "lessness are presented. It is argued that a single number will not be enough \n",
      "to understand homelessness and monitor progress in tackling it. More \n",
      "research and more work to improve information on homelessness at national levels will be needed before we can achieve comparable numbers at EU level.\n",
      ">>Keywords _ Data, d\n"
     ]
    }
   ],
   "source": [
    "# Import the required library for PDF reading\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from all pages of a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    # Open the PDF file in binary read mode\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        # Loop through each page and extract text\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Chunks text into segments with a specified overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    # Loop through the text and create overlapping chunks\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        # Move the start position forward by the chunk size minus the overlap\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# --- Your Original Logic ---\n",
    "\n",
    "# Define the path to the PDF file\n",
    "# IMPORTANT: Make sure this path is correct on your system\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/PHD/ISEM 770/Class Code SAT/Homelessness.pdf\"\n",
    "\n",
    "# Extract text from the PDF file by calling the function we defined\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text by calling the function we defined\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "# Print only the first chunk if it exists\n",
    "if text_chunks:\n",
    "    print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 65 embeddings.\n",
      "\n",
      "Embedding for the first chunk (first 5 values):\n",
      "[-0.037515923380851746, 0.012890207581222057, 0.06607704609632492, 0.017732873558998108, 0.032095909118652344]\n"
     ]
    }
   ],
   "source": [
    "# Make sure to install the library: pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# It's recommended to set your API key as an environment variable\n",
    "# but for clarity in this example, we'll initialize it here.\n",
    "# client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "client = OpenAI() # This works if OPENAI_API_KEY is set in your environment\n",
    "\n",
    "def create_embeddings(text_chunks, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for a list of text chunks using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (list[str]): The list of input texts for which embeddings are to be created.\n",
    "    model (str): The OpenAI model to be used. Default is \"text-embedding-3-small\".\n",
    "\n",
    "    Returns:\n",
    "    list: A list of embedding objects from the OpenAI API.\n",
    "    \"\"\"\n",
    "    # The 'input' parameter for the OpenAI API can take a list of strings directly\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text_chunks\n",
    "    )\n",
    "\n",
    "    # The embeddings are located in response.data\n",
    "    return response.data\n",
    "\n",
    "# Assume 'text_chunks' is a list of strings from your previous code\n",
    "# Example: text_chunks = [\"This is the first sentence.\", \"This is the second one.\"]\n",
    "\n",
    "# Create embeddings for the text chunks\n",
    "embeddings_data = create_embeddings(text_chunks)\n",
    "\n",
    "# Print the number of embeddings created\n",
    "print(f\"Successfully created {len(embeddings_data)} embeddings.\")\n",
    "\n",
    "# Print the embedding for the first text chunk (optional)\n",
    "if embeddings_data:\n",
    "    print(\"\\nEmbedding for the first chunk (first 5 values):\")\n",
    "    print(embeddings_data[0].embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 65 embeddings with model 'text-embedding-3-small'.\n",
      "\n",
      "First 5 values of the first embedding:\n",
      "[-0.037481676787137985, 0.012894639745354652, 0.06609976291656494, 0.01773896999657154, 0.032106947153806686]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# 2. Initialize the client\n",
    "# The client automatically looks for the OPENAI_API_KEY environment variable.\n",
    "client = OpenAI()\n",
    "\n",
    "# Assume 'text_chunks' is a list of strings from your previous code\n",
    "# Example: text_chunks = [\"This is the first sentence.\", \"This is the second one.\"]\n",
    "\n",
    "# 3. Create embeddings using the specified OpenAI model\n",
    "model_name = \"text-embedding-3-small\"\n",
    "response = client.embeddings.create(\n",
    "    model=model_name,\n",
    "    input=text_chunks\n",
    ")\n",
    "\n",
    "# 4. Extract the embedding vectors from the response object\n",
    "# The actual embeddings are in the `.data` attribute of the response.\n",
    "embeddings = [embedding_item.embedding for embedding_item in response.data]\n",
    "\n",
    "# Check the first embedding's first few values\n",
    "if embeddings:\n",
    "    print(f\"Successfully created {len(embeddings)} embeddings with model '{model_name}'.\")\n",
    "    print(\"\\nFirst 5 values of the first embedding:\")\n",
    "    print(embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query: str, text_chunks: list[str], embeddings: list[list[float]], k: int):\n",
    "    \"\"\"\n",
    "    Performs semantic search using a query, text chunks, and their embeddings.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)[0].embedding\n",
    "\n",
    "    # Calculate similarity scores between the query and each text chunk\n",
    "    similarity_scores = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        embeddings\n",
    "    )[0]\n",
    "\n",
    "    # Get the indices of the top k scores\n",
    "    top_k_indices = np.argsort(similarity_scores)[-k:][::-1]\n",
    "\n",
    "    # Return the corresponding text chunks using the full variable name\n",
    "    #\n",
    "    # OLD, INCORRECT line:\n",
    "    # return [text_chunks[i] for i in top_\n",
    "    #\n",
    "    # NEW, CORRECT line:\n",
    "    return [text_chunks[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      5\u001b[39m system_prompt = (\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou are an AI assistant that strictly answers based on the given context. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIf the answer cannot be derived directly from the provided context, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrespond with: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mI do not have enough information to answer that.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2. Define the user prompt by combining context and the query\u001b[39;00m\n\u001b[32m     12\u001b[39m context_for_prompt = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     13\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContext \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=====================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtop_chunks\u001b[49m)\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m user_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_for_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Your function definition is correct\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'top_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume 'client' is your initialized OpenAI client.\n",
    "# Assume 'top_chunks' and 'query' are defined from previous steps.\n",
    "\n",
    "# 1. Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant that strictly answers based on the given context. \"\n",
    "    \"If the answer cannot be derived directly from the provided context, \"\n",
    "    \"respond with: 'I do not have enough information to answer that.'\"\n",
    ")\n",
    "\n",
    "# 2. Define the user prompt by combining context and the query\n",
    "context_for_prompt = \"\\n\".join(\n",
    "    f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\"\n",
    "    for i, chunk in enumerate(top_chunks)\n",
    ")\n",
    "user_prompt = f\"{context_for_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "\n",
    "# Your function definition is correct\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the OpenAI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The OpenAI model to be used for generating the response.\n",
    "                 Default is now \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "# Generate AI response using the defined prompts\n",
    "ai_response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "# Print the final response content from the AI\n",
    "print(ai_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5\n",
      "The AI response provides a general understanding of Explainable AI (XAI) and its importance, but it lacks the specific details about providing insights into AI decision-making and ensuring fairness in AI systems.\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
