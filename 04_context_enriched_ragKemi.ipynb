{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Context-Enriched Retrieval in RAG\n",
    "Retrieval-Augmented Generation (RAG) enhances AI responses by retrieving relevant knowledge from external sources. Traditional retrieval methods return isolated text chunks, which can lead to incomplete answers.\n",
    "\n",
    "To address this, we introduce Context-Enriched Retrieval, which ensures that retrieved information includes neighboring chunks for better coherence.\n",
    "\n",
    "Steps in This Notebook:\n",
    "- Data Ingestion: Extract text from a PDF.\n",
    "- Chunking with Overlapping Context: Split text into overlapping chunks to preserve context.\n",
    "- Embedding Creation: Convert text chunks into numerical representations.\n",
    "- Context-Aware Retrieval: Retrieve relevant chunks along with their neighbors for better completeness.\n",
    "- Response Generation: Use a language model to generate responses based on retrieved context.\n",
    "- Evaluation: Assess the model's response accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Defining and Measuring Homelessness\n",
      "Volker Busch-Geertsema\n",
      "GISS, Germany\n",
      ">> Abstract_ Substantial progress has been made at EU level on defining home-\n",
      "lessness. The European Typology on Homelessness and Housing Exclusion \n",
      "(ETHOS) is widely accepted in almost all European countries (and beyond) as \n",
      "a useful conceptual framework and almost everywhere definitions at national \n",
      "level (though often not identical with ETHOS) are discussed in relation to this \n",
      "typology. The development and some of th\n"
     ]
    }
   ],
   "source": [
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = '/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf'\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text[:500])  # print the first 500 characters to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 65\n",
      "\n",
      "First text chunk:\n",
      "19\n",
      "Defining and Measuring Homelessness\n",
      "Volker Busch-Geertsema\n",
      "GISS, Germany\n",
      ">> Abstract_ Substantial progress has been made at EU level on defining home-\n",
      "lessness. The European Typology on Homelessness and Housing Exclusion \n",
      "(ETHOS) is widely accepted in almost all European countries (and beyond) as \n",
      "a useful conceptual framework and almost everywhere definitions at national \n",
      "level (though often not identical with ETHOS) are discussed in relation to this \n",
      "typology. The development and some of the remaining controversial issues \n",
      "concerning ETHOS and a reduced version of it are discussed in this chapter. \n",
      "Furthermore essential reasons and different approaches to measure home-\n",
      "lessness are presented. It is argued that a single number will not be enough \n",
      "to understand homelessness and monitor progress in tackling it. More \n",
      "research and more work to improve information on homelessness at national \n",
      "levels will be needed before we can achieve comparable numbers at EU level.\n",
      ">> Keywords_ Data,\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"Homelessness.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Default is \"BAAI/bge-en-icl\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the OpenAI API containing the embeddings.\n",
    "    \"\"\"\n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response  # Return the response containing the embeddings\n",
    "\n",
    "# Create embeddings for the text chunks\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Context-Aware Semantic Search\n",
    "We modify retrieval to include neighboring chunks for better context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant chunk along with its neighboring chunks.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    embeddings (List[dict]): List of chunk embeddings.\n",
    "    k (int): Number of relevant chunks to retrieve.\n",
    "    context_size (int): Number of neighboring chunks to include.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Relevant text chunks with contextual information.\n",
    "    \"\"\"\n",
    "    # Convert the query into an embedding vector\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Compute similarity scores between query and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        # Calculate cosine similarity between the query embedding and current chunk embedding\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        # Store the index and similarity score as a tuple\n",
    "        similarity_scores.append((i, similarity_score))\n",
    "\n",
    "    # Sort chunks by similarity score in descending order (highest similarity first)\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the index of the most relevant chunk\n",
    "    top_index = similarity_scores[0][0]\n",
    "\n",
    "    # Define the range for context inclusion\n",
    "    # Ensure we don't go below 0 or beyond the length of text_chunks\n",
    "    start = max(0, top_index - context_size)\n",
    "    end = min(len(text_chunks), top_index + context_size + 1)\n",
    "\n",
    "    # Return the relevant chunk along with its neighboring context chunks\n",
    "    return [text_chunks[i] for i in range(start, end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query with Context Retrieval\n",
    "We now test the context-enriched retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the ETHOS typology?\n",
      "Context 1:\n",
      " homelessness, while people living in insecure and/or inadequate housing \n",
      "and/or in social isolation might also be affected by exclusion from one or two domains, \n",
      "but their situation is classified under ‘housing exclusion’ rather than ‘homelessness’.\n",
      "On the basis of this conceptional understanding and to try to grasp the varying \n",
      "practices in different EU countries, the ETHOS typology was developed, which \n",
      "relates, in its most recent version, thirteen different operational categories and \n",
      "twenty-four different living situations to the four conceptional categories: roofless, \n",
      "houseless, insecure housing and inadequate housing.4 See Table 1.2.\n",
      "4\t\n",
      "Apart from documenting progress concerning the measurement of homelessness in different \n",
      "EU countries and reporting on the latest available data, the forth and fifth reviews of statistics \n",
      "(Edgar and Meert, 2005, 2006) focused on developing and refining the ETHOS definition and \n",
      "considering the measurement issues involved in greater detail. \n",
      "\n",
      "24\n",
      "=====================================\n",
      "Context 2:\n",
      "he forth and fifth reviews of statistics \n",
      "(Edgar and Meert, 2005, 2006) focused on developing and refining the ETHOS definition and \n",
      "considering the measurement issues involved in greater detail. \n",
      "\n",
      "24\n",
      "Homelessness Research in Europe\n",
      "Table 1.2 ETHOS – European typology on homelessness and housing exclusion\n",
      "Conceptual \n",
      "category\n",
      "Operational category\n",
      "Living situation\n",
      "ROOFLESS\n",
      "1\n",
      "People living rough\n",
      "1.1\n",
      "Public space or external space\n",
      "2\n",
      "People staying in a night shelter 2.1\n",
      "Night shelter\n",
      "HOUSELESS\n",
      "3\n",
      "People in accommodation  \n",
      "for the homeless\n",
      "3.1\n",
      "3.2\n",
      "3.3\n",
      "Homeless hostel\n",
      "Temporary accommodation\n",
      "Transitional supported \n",
      "accommodation\n",
      "4\n",
      "People in a women’s shelter\n",
      "4.1\n",
      "Women’s shelter accommodation\n",
      "5\n",
      "People in accommodation  \n",
      "for immigrants\n",
      "5.1\n",
      "5.2\n",
      "Temporary accommodation, \n",
      "reception centres \n",
      "Migrant workers’ accommodation\n",
      "6\n",
      "People due to be released  \n",
      "from institutions\n",
      "6.1\n",
      "6.2\n",
      "6.3\n",
      "Penal institutions\n",
      "Medical institutions\n",
      "Children’s institutions/homes\n",
      "7\n",
      "People receiving longer-term \n",
      "support (due to \n",
      "=====================================\n",
      "Context 3:\n",
      "orkers’ accommodation\n",
      "6\n",
      "People due to be released  \n",
      "from institutions\n",
      "6.1\n",
      "6.2\n",
      "6.3\n",
      "Penal institutions\n",
      "Medical institutions\n",
      "Children’s institutions/homes\n",
      "7\n",
      "People receiving longer-term \n",
      "support (due to homelessness)\n",
      "7.1\n",
      "7.2\n",
      "Residential care  \n",
      "for older homeless people\n",
      "Supported accommodation  \n",
      "for formerly homeless persons\n",
      "INSECURE\n",
      "8\n",
      "People living in  \n",
      "insecure accommodation\n",
      "8.1\n",
      "8.2\n",
      "8.3\n",
      "Temporarily with family/friends\n",
      "No legal (sub)tenancy\n",
      "Illegal occupation of land \n",
      "9\n",
      "People living  \n",
      "under threat of eviction\n",
      "9.1\n",
      "9.2\n",
      "Legal orders enforced (rented)\n",
      "Repossession orders (owned)\n",
      "10\n",
      "People living  \n",
      "under threat of violence\n",
      "10.1\n",
      "Police recorded incidents\n",
      "INADEQUATE\n",
      "11\n",
      "People living in temporary/\n",
      "non-conventional structures\n",
      "11.1\n",
      "11.2\n",
      "11.3\n",
      "Mobile homes\n",
      "Non-conventional building\n",
      "Temporary structure\n",
      "12\n",
      "People living in unfit housing\n",
      "12.1\n",
      "Occupied dwelling  \n",
      "unfit for habitation \n",
      "13\n",
      "People living  \n",
      "in extreme overcrowding\n",
      "13.1\n",
      "Highest national norm  \n",
      "of overcrowding\n",
      "Source: Edgar, 2009, p.73.\n",
      "\n",
      "25\n",
      "T\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Load the validation dataset from a JSON file\n",
    "with open('/Users/kekunkoya/Desktop/ISEM 770 Class Project/valh.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first question from the dataset to use as our query\n",
    "query = data[0]['question']\n",
    "\n",
    "# Retrieve the most relevant chunk and its neighboring chunks for context\n",
    "# Parameters:\n",
    "# - query: The question we're searching for\n",
    "# - text_chunks: Our text chunks extracted from the PDF\n",
    "# - response.data: The embeddings of our text chunks\n",
    "# - k=1: Return the top match\n",
    "# - context_size=1: Include 1 chunk before and after the top match for context\n",
    "top_chunks = context_enriched_search(query, text_chunks, response.data, k=1, context_size=1)\n",
    "\n",
    "# Print the query for reference\n",
    "print(\"Query:\", query)\n",
    "# Print each retrieved chunk with a heading and separator\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Using Retrieved Context\n",
    "We now generate a response using LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the Homelessness AI.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"gpt-4o-mini\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-new-specific-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
