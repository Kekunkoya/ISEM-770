{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Hierarchical Indices for RAG\n",
    "\n",
    "In this notebook, I implement a hierarchical indexing approach for RAG systems. This technique improves retrieval by using a two-tier search method: first identifying relevant document sections through summaries, then retrieving specific details from those sections.\n",
    "\n",
    "Traditional RAG approaches treat all text chunks equally, which can lead to:\n",
    "\n",
    "- Lost context when chunks are too small\n",
    "- Irrelevant results when the document collection is large\n",
    "- Inefficient searches across the entire corpus\n",
    "\n",
    "Hierarchical retrieval solves these problems by:\n",
    "\n",
    "- Creating concise summaries for larger document sections\n",
    "- First searching these summaries to identify relevant sections\n",
    "- Then retrieving detailed information only from those sections\n",
    "- Maintaining context while preserving specific details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file with page separation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of pages with text content and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    pages = []  # Initialize an empty list to store the pages with text content\n",
    "    \n",
    "    # Iterate over each page in the PDF\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]  # Get the current page\n",
    "        text = page.get_text()  # Extract text from the current page\n",
    "        \n",
    "        # Skip pages with very little text (less than 50 characters)\n",
    "        if len(text.strip()) > 50:\n",
    "            # Append the page text and metadata to the list\n",
    "            pages.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # Source file path\n",
    "                    \"page\": page_num + 1  # Page number (1-based index)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\n",
    "    return pages  # Return the list of pages with text content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, metadata, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks while preserving metadata.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        metadata (Dict): Metadata to preserve\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of text chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # Extract the chunk of text\n",
    "        \n",
    "        # Skip very small chunks (less than 50 characters)\n",
    "        if chunk_text and len(chunk_text.strip()) > 50:\n",
    "            # Create a copy of metadata and add chunk-specific info\n",
    "            chunk_metadata = metadata.copy()\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_index\": len(chunks),  # Index of the chunk\n",
    "                \"start_char\": i,  # Start character index of the chunk\n",
    "                \"end_char\": i + len(chunk_text),  # End character index of the chunk\n",
    "                \"is_summary\": False  # Flag indicating this is not a summary\n",
    "            })\n",
    "            \n",
    "            # Append the chunk with its metadata to the list\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": chunk_metadata\n",
    "            })\n",
    "    \n",
    "    return chunks  # Return the list of chunks with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store vector embeddings\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text content\n",
    "            embedding (List[float]): Vector embedding\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            filter_func (callable, optional): Function to filter results\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return an empty list if there are no vectors\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Skip if doesn't pass the filter\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text content\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": float(score)  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    return all_embeddings  # Return all embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_summary(page_text):\n",
    "    \"\"\"\n",
    "    Generate a concise summary of a page.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): Text content of the page\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated summary\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the summarization model\n",
    "    system_prompt = \"\"\"You are an expert summarization system.\n",
    "    Create a detailed summary of the provided text. \n",
    "    Focus on capturing the main topics, key information, and important facts.\n",
    "    Your summary should be comprehensive enough to understand what the page contains\n",
    "    but more concise than the original.\"\"\"\n",
    "\n",
    "    # Truncate input text if it exceeds the maximum token limit\n",
    "    max_tokens = 6000\n",
    "    truncated_text = page_text[:max_tokens] if len(page_text) > max_tokens else page_text\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the summary\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize this text:\\n\\n{truncated_text}\"}  # User message with the text to summarize\n",
    "        ],\n",
    "        temperature=0.3  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated summary content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document into hierarchical indices.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: Summary and detailed vector stores\n",
    "    \"\"\"\n",
    "    # Extract pages from PDF\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create summaries for each page\n",
    "    print(\"Generating page summaries...\")\n",
    "    summaries = []\n",
    "    for i, page in enumerate(pages):\n",
    "        print(f\"Summarizing page {i+1}/{len(pages)}...\")\n",
    "        summary_text = generate_page_summary(page[\"text\"])\n",
    "        \n",
    "        # Create summary metadata\n",
    "        summary_metadata = page[\"metadata\"].copy()\n",
    "        summary_metadata.update({\"is_summary\": True})\n",
    "        \n",
    "        # Append the summary text and metadata to the summaries list\n",
    "        summaries.append({\n",
    "            \"text\": summary_text,\n",
    "            \"metadata\": summary_metadata\n",
    "        })\n",
    "    \n",
    "    # Create detailed chunks for each page\n",
    "    detailed_chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the detailed_chunks list with the chunks from the current page\n",
    "        detailed_chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(detailed_chunks)} detailed chunks\")\n",
    "    \n",
    "    # Create embeddings for summaries\n",
    "    print(\"Creating embeddings for summaries...\")\n",
    "    summary_texts = [summary[\"text\"] for summary in summaries]\n",
    "    summary_embeddings = create_embeddings(summary_texts)\n",
    "    \n",
    "    # Create embeddings for detailed chunks\n",
    "    print(\"Creating embeddings for detailed chunks...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in detailed_chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Create vector stores\n",
    "    summary_store = SimpleVectorStore()\n",
    "    detailed_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add summaries to summary store\n",
    "    for i, summary in enumerate(summaries):\n",
    "        summary_store.add_item(\n",
    "            text=summary[\"text\"],\n",
    "            embedding=summary_embeddings[i],\n",
    "            metadata=summary[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Add chunks to detailed store\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        detailed_store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk_embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"Created vector stores with {len(summaries)} summaries and {len(detailed_chunks)} chunks\")\n",
    "    return summary_store, detailed_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Retrieve information using hierarchical indices.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        summary_store (SimpleVectorStore): Store of document summaries\n",
    "        detailed_store (SimpleVectorStore): Store of detailed chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved chunks with relevance scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing hierarchical retrieval for query: {query}\")\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # First, retrieve relevant summaries\n",
    "    summary_results = summary_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_summaries\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(summary_results)} relevant summaries\")\n",
    "    \n",
    "    # Collect pages from relevant summaries\n",
    "    relevant_pages = [result[\"metadata\"][\"page\"] for result in summary_results]\n",
    "    \n",
    "    # Create a filter function to only keep chunks from relevant pages\n",
    "    def page_filter(metadata):\n",
    "        return metadata[\"page\"] in relevant_pages\n",
    "    \n",
    "    # Then, retrieve detailed chunks from only those relevant pages\n",
    "    detailed_results = detailed_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_chunks * len(relevant_pages),\n",
    "        filter_func=page_filter\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(detailed_results)} detailed chunks from relevant pages\")\n",
    "    \n",
    "    # For each result, add which summary/page it came from\n",
    "    for result in detailed_results:\n",
    "        page = result[\"metadata\"][\"page\"]\n",
    "        matching_summaries = [s for s in summary_results if s[\"metadata\"][\"page\"] == page]\n",
    "        if matching_summaries:\n",
    "            result[\"summary\"] = matching_summaries[0][\"text\"]\n",
    "    \n",
    "    return detailed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        retrieved_chunks (List[Dict]): Retrieved chunks from hierarchical search\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Extract text from chunks and prepare context parts\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        page_num = chunk[\"metadata\"][\"page\"]  # Get the page number from metadata\n",
    "        context_parts.append(f\"[Page {page_num}]: {chunk['text']}\")  # Format the chunk text with page number\n",
    "    \n",
    "    # Combine all context parts into a single context string\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Define the system message to guide the AI assistant\n",
    "    system_message = \"\"\"You are a helpful AI assistant answering questions based on the provided context.\n",
    "Use the information from the context to answer the user's question accurately.\n",
    "If the context doesn't contain relevant information, acknowledge that.\n",
    "Include page numbers when referencing specific information.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {query}\"}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.2  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Hierarchical Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, \n",
    "                    k_summaries=3, k_chunks=5, regenerate=False):\n",
    "    \"\"\"\n",
    "    Complete hierarchical RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        regenerate (bool): Whether to regenerate vector stores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Create store filenames for caching\n",
    "    summary_store_file = f\"{os.path.basename(pdf_path)}_summary_store.pkl\"\n",
    "    detailed_store_file = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\"\n",
    "    \n",
    "    # Process document and create stores if needed\n",
    "    if regenerate or not os.path.exists(summary_store_file) or not os.path.exists(detailed_store_file):\n",
    "        print(\"Processing document and creating vector stores...\")\n",
    "        # Process the document to create hierarchical indices and vector stores\n",
    "        summary_store, detailed_store = process_document_hierarchically(\n",
    "            pdf_path, chunk_size, chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Save the summary store to a file for future use\n",
    "        with open(summary_store_file, 'wb') as f:\n",
    "            pickle.dump(summary_store, f)\n",
    "        \n",
    "        # Save the detailed store to a file for future use\n",
    "        with open(detailed_store_file, 'wb') as f:\n",
    "            pickle.dump(detailed_store, f)\n",
    "    else:\n",
    "        # Load existing summary store from file\n",
    "        print(\"Loading existing vector stores...\")\n",
    "        with open(summary_store_file, 'rb') as f:\n",
    "            summary_store = pickle.load(f)\n",
    "        \n",
    "        # Load existing detailed store from file\n",
    "        with open(detailed_store_file, 'rb') as f:\n",
    "            detailed_store = pickle.load(f)\n",
    "    \n",
    "    # Retrieve relevant chunks hierarchically using the query\n",
    "    retrieved_chunks = retrieve_hierarchically(\n",
    "        query, summary_store, detailed_store, k_summaries, k_chunks\n",
    "    )\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return results including the query, response, retrieved chunks, and counts of summaries and detailed chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"summary_count\": len(summary_store.texts),\n",
    "        \"detailed_count\": len(detailed_store.texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (Non-Hierarchical) RAG for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k=15):\n",
    "    \"\"\"\n",
    "    Standard RAG pipeline without hierarchical retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k (int): Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Extract pages from the PDF document\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create chunks directly from all pages\n",
    "    chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the chunks list with the chunks from the current page\n",
    "        chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks for standard RAG\")\n",
    "    \n",
    "    # Create a vector store to hold the chunks\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Create embeddings for the chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Add chunks to the vector store\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve the most relevant chunks based on the query embedding\n",
    "    retrieved_chunks = store.similarity_search(query_embedding, k=k)\n",
    "    print(f\"Retrieved {len(retrieved_chunks)} chunks with standard RAG\")\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return the results including the query, response, and retrieved chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(query, pdf_path, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare hierarchical and standard RAG approaches.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        reference_answer (str, optional): Reference answer for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing RAG approaches for query: {query} ===\")\n",
    "    \n",
    "    # Run hierarchical RAG\n",
    "    print(\"\\nRunning hierarchical RAG...\")\n",
    "    hierarchical_result = hierarchical_rag(query, pdf_path)\n",
    "    hier_response = hierarchical_result[\"response\"]\n",
    "    \n",
    "    # Run standard RAG\n",
    "    print(\"\\nRunning standard RAG...\")\n",
    "    standard_result = standard_rag(query, pdf_path)\n",
    "    std_response = standard_result[\"response\"]\n",
    "    \n",
    "    # Compare results from hierarchical and standard RAG\n",
    "    comparison = compare_responses(query, hier_response, std_response, reference_answer)\n",
    "    \n",
    "    # Return a dictionary with the comparison results\n",
    "    return {\n",
    "        \"query\": query,  # The original query\n",
    "        \"hierarchical_response\": hier_response,  # Response from hierarchical RAG\n",
    "        \"standard_response\": std_response,  # Response from standard RAG\n",
    "        \"reference_answer\": reference_answer,  # Reference answer for evaluation\n",
    "        \"comparison\": comparison,  # Comparison analysis\n",
    "        \"hierarchical_chunks_count\": len(hierarchical_result[\"retrieved_chunks\"]),  # Number of chunks retrieved by hierarchical RAG\n",
    "        \"standard_chunks_count\": len(standard_result[\"retrieved_chunks\"])  # Number of chunks retrieved by standard RAG\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, hierarchical_response, standard_response, reference=None):\n",
    "    \"\"\"\n",
    "    Compare responses from hierarchical and standard RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        hierarchical_response (str): Response from hierarchical RAG\n",
    "        standard_response (str): Response from standard RAG\n",
    "        reference (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate the responses\n",
    "    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems. \n",
    "Compare the two responses to the same query, one generated using hierarchical retrieval\n",
    "and the other using standard retrieval.\n",
    "\n",
    "Evaluate them based on:\n",
    "1. Accuracy: Which response provides more factually correct information?\n",
    "2. Comprehensiveness: Which response better covers all aspects of the query?\n",
    "3. Coherence: Which response has better logical flow and organization?\n",
    "4. Page References: Does either response make better use of page references?\n",
    "\n",
    "Be specific in your analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "\n",
    "    # Create the user prompt with the query and both responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Response from Hierarchical RAG:\n",
    "{hierarchical_response}\n",
    "\n",
    "Response from Standard RAG:\n",
    "{standard_response}\"\"\"\n",
    "\n",
    "    # If a reference answer is provided, include it in the user prompt\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\"\"\"\n",
    "\n",
    "    # Add the final instruction to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "\n",
    "Please provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the comparison analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated comparison analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation with multiple test queries.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for queries\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    \n",
    "    # Iterate over each query in the test queries\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"Query: {query}\")  # Print the current query\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]  # Retrieve the reference answer for the current query\n",
    "        \n",
    "        # Compare hierarchical and standard RAG approaches\n",
    "        result = compare_approaches(query, pdf_path, reference)\n",
    "        results.append(result)  # Append the result to the results list\n",
    "    \n",
    "    # Generate overall analysis of the evaluation results\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,  # Return the individual results\n",
    "        \"overall_analysis\": overall_analysis  # Return the overall analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from individual query evaluations\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate the results\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
    "Based on multiple test queries, provide an overall analysis comparing hierarchical RAG \n",
    "with standard RAG.\n",
    "\n",
    "Focus on:\n",
    "1. When hierarchical retrieval performs better and why\n",
    "2. When standard retrieval performs better and why\n",
    "3. The overall strengths and weaknesses of each approach\n",
    "4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of the evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Hierarchical chunks: {result['hierarchical_chunks_count']}, Standard chunks: {result['standard_chunks_count']}\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # Define the user prompt with the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations comparing hierarchical vs standard RAG across {len(results)} queries, \n",
    "provide an overall analysis of these two approaches:\n",
    "\n",
    "{evaluations_summary}\n",
    "\n",
    "Please provide a comprehensive analysis of the relative strengths and weaknesses of hierarchical RAG \n",
    "compared to standard RAG, with specific focus on retrieval quality and response generation.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the overall analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the evaluations summary\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated overall analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Hierarchical and Standard RAG Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document and creating vector stores...\n",
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Generating page summaries...\n",
      "Summarizing page 1/15...\n",
      "Summarizing page 2/15...\n",
      "Summarizing page 3/15...\n",
      "Summarizing page 4/15...\n",
      "Summarizing page 5/15...\n",
      "Summarizing page 6/15...\n",
      "Summarizing page 7/15...\n",
      "Summarizing page 8/15...\n",
      "Summarizing page 9/15...\n",
      "Summarizing page 10/15...\n",
      "Summarizing page 11/15...\n",
      "Summarizing page 12/15...\n",
      "Summarizing page 13/15...\n",
      "Summarizing page 14/15...\n",
      "Summarizing page 15/15...\n",
      "Created 47 detailed chunks\n",
      "Creating embeddings for summaries...\n",
      "Creating embeddings for detailed chunks...\n",
      "Created vector stores with 15 summaries and 47 chunks\n",
      "Performing hierarchical retrieval for query: What are the key applications of transformer models in natural language processing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/d7p0227s7t73c173mgvx4kjc0000gn/T/ipykernel_89467/2918097221.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Add the similarity score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant summaries\n",
      "Retrieved 11 detailed chunks from relevant pages\n",
      "\n",
      "=== Response ===\n",
      "The provided context does not mention transformer models or their specific applications in natural language processing (NLP). Therefore, I cannot provide an answer based on the given information.\n",
      "Query: How do transformers handle sequential data compared to RNNs?\n",
      "\n",
      "=== Comparing RAG approaches for query: How do transformers handle sequential data compared to RNNs? ===\n",
      "\n",
      "Running hierarchical RAG...\n",
      "Loading existing vector stores...\n",
      "Performing hierarchical retrieval for query: How do transformers handle sequential data compared to RNNs?\n",
      "Retrieved 3 relevant summaries\n",
      "Retrieved 11 detailed chunks from relevant pages\n",
      "\n",
      "Running standard RAG...\n",
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Created 47 chunks for standard RAG\n",
      "Creating embeddings for chunks...\n",
      "Retrieved 15 chunks with standard RAG\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "### Overall Analysis of Hierarchical RAG vs. Standard RAG\n",
      "\n",
      "#### 1. When Hierarchical Retrieval Performs Better and Why\n",
      "Hierarchical RAG (Retrieval-Augmented Generation) tends to perform better in scenarios where the information is structured or can be logically organized into layers or categories. This is particularly beneficial in the following situations:\n",
      "\n",
      "- **Complex Queries**: When queries require nuanced understanding or multi-faceted responses, hierarchical retrieval can effectively break down the information into manageable chunks. For example, if a query involves multiple aspects of a topic (e.g., historical context, technical details, and applications), hierarchical retrieval can prioritize and retrieve relevant information from different layers of knowledge.\n",
      "\n",
      "- **Domain-Specific Knowledge**: In specialized fields where information is organized hierarchically (e.g., medical or legal domains), hierarchical RAG can leverage this structure to retrieve more relevant and contextually appropriate information.\n",
      "\n",
      "- **Contextual Relevance**: Hierarchical retrieval can maintain context better by organizing information in a way that reflects the relationships between concepts, which can lead to more coherent and contextually relevant responses.\n",
      "\n",
      "#### 2. When Standard Retrieval Performs Better and Why\n",
      "Standard RAG may outperform hierarchical retrieval in scenarios characterized by:\n",
      "\n",
      "- **Simplicity of Queries**: For straightforward queries that require direct answers or factual information, standard retrieval can quickly access and return relevant chunks without the need for complex organization. This is particularly effective for queries that do not require deep contextual understanding.\n",
      "\n",
      "- **Broader Information Needs**: When users are looking for a wide range of information or diverse perspectives on a topic, standard retrieval can provide a more comprehensive set of responses by pulling from a larger pool of unstructured data.\n",
      "\n",
      "- **Speed and Efficiency**: Standard retrieval systems may be faster in generating responses since they do not require the additional processing of organizing information hierarchically. This can be advantageous in time-sensitive situations.\n",
      "\n",
      "#### 3. Overall Strengths and Weaknesses of Each Approach\n",
      "\n",
      "**Hierarchical RAG:**\n",
      "- **Strengths:**\n",
      "  - Better at handling complex, multi-layered queries.\n",
      "  - Can provide more contextually relevant and coherent responses.\n",
      "  - Effective in specialized domains with structured knowledge.\n",
      "\n",
      "- **Weaknesses:**\n",
      "  - May struggle with simple queries, leading to irrelevant or incomplete responses.\n",
      "  - Requires more computational resources and time to organize and retrieve information.\n",
      "  - Potentially limited by the quality of the hierarchical structure used.\n",
      "\n",
      "**Standard RAG:**\n",
      "- **Strengths:**\n",
      "  - Quick and efficient for straightforward queries.\n",
      "  - Capable of retrieving a wide range of information from unstructured data.\n",
      "  - Less computationally intensive compared to hierarchical approaches.\n",
      "\n",
      "- **Weaknesses:**\n",
      "  - May produce less coherent responses for complex queries.\n",
      "  - Lacks the ability to maintain context across multiple layers of information.\n",
      "  - Can lead to information overload if too many irrelevant chunks are retrieved.\n",
      "\n",
      "#### 4. Recommendations for When to Use Each Approach\n",
      "\n",
      "- **Use Hierarchical RAG When:**\n",
      "  - The query is complex and requires a nuanced understanding of multiple concepts.\n",
      "  - The information domain is structured and benefits from hierarchical organization.\n",
      "  - Contextual relevance and coherence are critical for the response.\n",
      "\n",
      "- **Use Standard RAG When:**\n",
      "  - The query is straightforward and seeks direct answers or factual information.\n",
      "  - The user requires a broad overview or diverse perspectives on a topic.\n",
      "  - Speed and efficiency are prioritized over depth and context.\n",
      "\n",
      "### Conclusion\n",
      "In summary, the choice between hierarchical RAG and standard RAG should be guided by the nature of the query and the specific information needs of the user. Hierarchical RAG excels in complexity and context, while standard RAG is advantageous for simplicity and breadth. Understanding these strengths and weaknesses can help in selecting the most appropriate retrieval approach for a given situation.\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/AI_Information.pdf\"\n",
    "\n",
    "# Example query about AI for testing the hierarchical RAG approach\n",
    "query = \"What are the key applications of transformer models in natural language processing?\"\n",
    "result = hierarchical_rag(query, pdf_path)\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "# Test query for formal evaluation (using only one query as requested)\n",
    "test_queries = [\n",
    "    \"How do transformers handle sequential data compared to RNNs?\"\n",
    "]\n",
    "\n",
    "# Reference answer for the test query to enable comparison\n",
    "reference_answers = [\n",
    "    \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n",
    "]\n",
    "\n",
    "# Run the evaluation comparing hierarchical and standard RAG approaches\n",
    "evaluation_results = run_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print the overall analysis of the comparison\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document and creating vector stores...\n",
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf...\n",
      "Extracted 21 pages with content\n",
      "Generating page summaries...\n",
      "Summarizing page 1/21...\n",
      "Summarizing page 2/21...\n",
      "Summarizing page 3/21...\n",
      "Summarizing page 4/21...\n",
      "Summarizing page 5/21...\n",
      "Summarizing page 6/21...\n",
      "Summarizing page 7/21...\n",
      "Summarizing page 8/21...\n",
      "Summarizing page 9/21...\n",
      "Summarizing page 10/21...\n",
      "Summarizing page 11/21...\n",
      "Summarizing page 12/21...\n",
      "Summarizing page 13/21...\n",
      "Summarizing page 14/21...\n",
      "Summarizing page 15/21...\n",
      "Summarizing page 16/21...\n",
      "Summarizing page 17/21...\n",
      "Summarizing page 18/21...\n",
      "Summarizing page 19/21...\n",
      "Summarizing page 20/21...\n",
      "Summarizing page 21/21...\n",
      "Created 71 detailed chunks\n",
      "Creating embeddings for summaries...\n",
      "Creating embeddings for detailed chunks...\n",
      "Created vector stores with 21 summaries and 71 chunks\n",
      "Performing hierarchical retrieval for query: What have been done to prevent homelessness?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/d7p0227s7t73c173mgvx4kjc0000gn/T/ipykernel_89467/2918097221.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Add the similarity score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant summaries\n",
      "Retrieved 10 detailed chunks from relevant pages\n",
      "\n",
      "=== Response ===\n",
      "To prevent homelessness, several strategies and measures have been discussed in the context of homelessness research in Europe:\n",
      "\n",
      "1. **Monitoring and Information Gathering**: It is essential to have accurate information on the total number of homeless households, those living in temporary or inadequate housing, and individuals at risk of eviction. This data helps in understanding the scope of the issue and in formulating effective policies (Page 12).\n",
      "\n",
      "2. **Provision of Sustainable Accommodation**: Policies aimed at preventing homelessness emphasize the need for sustainable permanent accommodation for formerly homeless individuals. This includes ensuring that those who gain access to supported accommodation are tracked and supported (Page 12).\n",
      "\n",
      "3. **Political Commitment and Stakeholder Involvement**: A political commitment at the national level, along with the clarification of responsibilities among government departments and the involvement of key stakeholders (including municipalities and NGOs), is crucial for effective homelessness prevention strategies (Page 16).\n",
      "\n",
      "4. **Standardized Data Collection**: The inclusion of standardized questions about experiences of homelessness in national and European household surveys can provide comparable information on homelessness prevalence, which is vital for understanding and addressing the issue (Page 16).\n",
      "\n",
      "5. **Comprehensive Information Strategies**: Developing a comprehensive homelessness information strategy that utilizes multiple data sources is necessary for obtaining accurate national statistics on homelessness (Page 16).\n",
      "\n",
      "6. **Research on Service Outcomes**: More research is needed to evaluate the long-term effects of service provision on individuals who have experienced homelessness. Understanding the dynamics of homelessness and the effectiveness of interventions can inform better prevention strategies (Page 12).\n",
      "\n",
      "These measures highlight the importance of data, political will, and collaborative efforts in preventing homelessness effectively.\n",
      "Query: What are the strategies to prevent homelessness?\n",
      "\n",
      "=== Comparing RAG approaches for query: What are the strategies to prevent homelessness? ===\n",
      "\n",
      "Running hierarchical RAG...\n",
      "Loading existing vector stores...\n",
      "Performing hierarchical retrieval for query: What are the strategies to prevent homelessness?\n",
      "Retrieved 3 relevant summaries\n",
      "Retrieved 11 detailed chunks from relevant pages\n",
      "\n",
      "Running standard RAG...\n",
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf...\n",
      "Extracted 21 pages with content\n",
      "Created 71 chunks for standard RAG\n",
      "Creating embeddings for chunks...\n",
      "Retrieved 15 chunks with standard RAG\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "### Overall Analysis of Hierarchical RAG vs. Standard RAG\n",
      "\n",
      "#### 1. When Hierarchical Retrieval Performs Better and Why\n",
      "Hierarchical RAG (Retrieval-Augmented Generation) tends to perform better in scenarios where the information is structured or can be logically organized into categories. For example, in the query \"What are the strategies to prevent homelessness?\", hierarchical retrieval can effectively categorize strategies into broader themes (e.g., prevention, intervention, policy changes). This structured approach allows for:\n",
      "\n",
      "- **Contextual Relevance**: By organizing information hierarchically, the system can provide a more coherent and contextually relevant response, as it can draw from related chunks that are thematically linked.\n",
      "- **Focused Responses**: Hierarchical retrieval can filter out less relevant information, leading to more concise and targeted answers that directly address the query.\n",
      "\n",
      "#### 2. When Standard Retrieval Performs Better and Why\n",
      "Standard RAG may outperform hierarchical retrieval in situations where the information is less structured or when the query requires a broad range of diverse information. For instance, if the query is open-ended or exploratory, standard retrieval can:\n",
      "\n",
      "- **Diversity of Information**: Standard retrieval can pull from a wider array of sources without the constraints of a hierarchical structure, which can be beneficial for queries that require a variety of perspectives or solutions.\n",
      "- **Flexibility**: It can adapt to different types of queries, especially those that do not fit neatly into predefined categories, allowing for a more comprehensive exploration of the topic.\n",
      "\n",
      "#### 3. Overall Strengths and Weaknesses of Each Approach\n",
      "\n",
      "**Hierarchical RAG:**\n",
      "- **Strengths**:\n",
      "  - Provides structured and organized responses.\n",
      "  - Enhances contextual relevance by grouping related information.\n",
      "  - Can lead to more concise and focused answers.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - May struggle with queries that require diverse or non-categorical information.\n",
      "  - Risk of oversimplifying complex topics by forcing them into a hierarchical structure.\n",
      "  - Potentially limited by the quality of the hierarchical organization.\n",
      "\n",
      "**Standard RAG:**\n",
      "- **Strengths**:\n",
      "  - Offers flexibility and adaptability to various types of queries.\n",
      "  - Can retrieve a broader range of information, which is useful for exploratory queries.\n",
      "  - Less constrained by the need for a structured response.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - Responses may lack coherence and organization, leading to information overload.\n",
      "  - Risk of including irrelevant or tangential information that does not directly address the query.\n",
      "  - May require more post-processing to synthesize information into a coherent response.\n",
      "\n",
      "#### 4. Recommendations for When to Use Each Approach\n",
      "\n",
      "- **Use Hierarchical RAG When**:\n",
      "  - The query is specific and can be categorized into distinct themes or topics.\n",
      "  - The user requires a structured response that highlights relationships between different pieces of information.\n",
      "  - The information domain is well-defined and lends itself to hierarchical organization (e.g., educational content, policy frameworks).\n",
      "\n",
      "- **Use Standard RAG When**:\n",
      "  - The query is broad, exploratory, or open-ended, requiring a wide range of information.\n",
      "  - The user is looking for diverse perspectives or solutions that may not fit neatly into categories.\n",
      "  - The information domain is less structured or more dynamic, where flexibility is key.\n",
      "\n",
      "### Conclusion\n",
      "In summary, both hierarchical and standard RAG approaches have their unique strengths and weaknesses. The choice between them should be guided by the nature of the query and the type of information required. For structured, specific queries, hierarchical RAG is advantageous, while standard RAG is better suited for broad, exploratory inquiries. Understanding these dynamics can significantly enhance the effectiveness of information retrieval systems.\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# Example query about AI for testing the hierarchical RAG approach\n",
    "query = \"What have been done to prevent homelessness?\"\n",
    "result = hierarchical_rag(query, pdf_path)\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "# Test query for formal evaluation (using only one query as requested)\n",
    "test_queries = [\n",
    "     \"What are the strategies to prevent homelessness?\"\n",
    "]\n",
    "\n",
    "# Reference answer for the test query to enable comparison\n",
    "reference_answers = [\n",
    "    \"Prevent new incidences of homelessness through early intervention and support services.\",  \"Address and mitigate the underlying causes of homelessness, such as poverty, unemployment, and lack of affordable housing.\", \"Reduce the overall number of people experiencing homelessness via rapid rehousing and housing-first models.\", \"Minimize the negative social, health, and economic impacts on individuals and families currently experiencing homelessness.\", \"Ensure formerly homeless people maintain permanent, independent housing through ongoing support and follow-up services.\"\n",
    "]\n",
    "    \n",
    "\n",
    "# Run the evaluation comparing hierarchical and standard RAG approaches\n",
    "evaluation_results = run_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print the overall analysis of the comparison\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISEM 770 Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
