{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Fusion Retrieval: Combining Vector and Keyword Search\n",
    "\n",
    "In this notebook, I implement a fusion retrieval system that combines the strengths of semantic vector search with keyword-based BM25 retrieval. This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n",
    "\n",
    "## Why Fusion Retrieval Matters\n",
    "\n",
    "Traditional RAG systems typically rely on vector search alone, but this has limitations:\n",
    "\n",
    "- Vector search excels at semantic similarity but may miss exact keyword matches\n",
    "- Keyword search is great for specific terms but lacks semantic understanding\n",
    "- Different queries perform better with different retrieval methods\n",
    "\n",
    "Fusion retrieval gives us the best of both worlds by:\n",
    "\n",
    "- Performing both vector-based and keyword-based retrieval\n",
    "- Normalizing the scores from each approach\n",
    "- Combining them with a weighted formula\n",
    "- Ranking documents based on the combined score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # Get the page object\n",
    "        text += page.get_text()  # Extract text from the page and append to the text string\n",
    "    \n",
    "    return text  # Return the extracted text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with text and metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunk_data = {\n",
    "                \"text\": chunk,  # The chunk text\n",
    "                \"metadata\": {\n",
    "                    \"start_char\": i,  # Start character index of the chunk\n",
    "                    \"end_char\": i + len(chunk)  # End character index of the chunk\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk_data)  # Add the chunk data to the list\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    \n",
    "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "    \n",
    "    # Otherwise return all embeddings\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding vector\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of text items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],  # Extract text from item\n",
    "                embedding=embedding,  # Use corresponding embedding\n",
    "                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n",
    "            )\n",
    "    \n",
    "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding with similarity scores.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = cosine_similarity([query_vector], [vector])[0][0]  # Compute cosine similarity\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results with scores\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Retrieve text by index\n",
    "                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n",
    "                \"similarity\": float(score)  # Add similarity score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_documents(self):\n",
    "        \"\"\"\n",
    "        Get all documents in the store.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All documents\n",
    "        \"\"\"\n",
    "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(chunks):\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        BM25Okapi: A BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from each chunk\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    tokenized_docs = [text.split() for text in texts]\n",
    "    \n",
    "    # Create the BM25 index using the tokenized documents\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Print the number of documents in the BM25 index\n",
    "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
    "    \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(bm25, chunks, query, k=5):\n",
    "    \"\"\"\n",
    "    Search the BM25 index with a query.\n",
    "    \n",
    "    Args:\n",
    "        bm25 (BM25Okapi): BM25 index\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        query (str): Query string\n",
    "        k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results with scores\n",
    "    \"\"\"\n",
    "    # Tokenize the query by splitting it into individual words\n",
    "    query_tokens = query.split()\n",
    "    \n",
    "    # Get BM25 scores for the query tokens against the indexed documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Initialize an empty list to store results with their scores\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the scores and corresponding chunks\n",
    "    for i, score in enumerate(scores):\n",
    "        # Create a copy of the metadata to avoid modifying the original\n",
    "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
    "        # Add index to metadata\n",
    "        metadata[\"index\"] = i\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": chunks[i][\"text\"],\n",
    "            \"metadata\": metadata,  # Add metadata with index\n",
    "            \"bm25_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort the results by BM25 score in descending order\n",
    "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining vector-based and BM25 search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        chunks (List[Dict]): Original text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of results to return\n",
    "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results based on combined scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing fusion retrieval for query: {query}\")\n",
    "    \n",
    "    # Define small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_embedding = create_embeddings(query)  # Create embedding for the query\n",
    "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n",
    "    \n",
    "    # Get BM25 search results\n",
    "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n",
    "    \n",
    "    # Create dictionaries to map document index to score\n",
    "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
    "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
    "    \n",
    "    # Ensure all documents have scores for both methods\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    combined_results = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n",
    "        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n",
    "        combined_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"vector_score\": vector_score,\n",
    "            \"bm25_score\": bm25_score,\n",
    "            \"index\": i\n",
    "        })\n",
    "    \n",
    "    # Extract scores as arrays\n",
    "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
    "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
    "    \n",
    "    # Normalize scores\n",
    "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "    \n",
    "    # Compute combined scores\n",
    "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
    "    \n",
    "    # Add combined scores to results\n",
    "    for i, score in enumerate(combined_scores):\n",
    "        combined_results[i][\"combined_score\"] = float(score)\n",
    "    \n",
    "    # Sort by combined score (descending)\n",
    "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    \n",
    "    # Return top k results\n",
    "    top_results = combined_results[:k]\n",
    "    \n",
    "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Clean the extracted text to remove extra whitespace and special characters\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Split the cleaned text into overlapping chunks\n",
    "    chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract the text content from each chunk for embedding creation\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    \n",
    "    # Create embeddings for the chunk texts\n",
    "    embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add the chunks and their embeddings to the vector store\n",
    "    vector_store.add_items(chunks, embeddings)\n",
    "    print(f\"Added {len(chunks)} items to vector store\")\n",
    "    \n",
    "    # Create a BM25 index from the chunks\n",
    "    bm25_index = create_bm25_index(chunks)\n",
    "    \n",
    "    # Return the chunks, vector store, and BM25 index\n",
    "    return chunks, vector_store, bm25_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context from retrieved documents\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n",
    "\n",
    "    # Format the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please answer the question based on the provided context.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.3  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Answer a query using fusion RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results including retrieved documents and response\n",
    "    \"\"\"\n",
    "    # Retrieve documents using fusion retrieval method\n",
    "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_only_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only vector-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve documents using vector-based similarity search\n",
    "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only BM25-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Retrieve documents using BM25 search\n",
    "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare different retrieval methods for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
    "    \n",
    "    # Run vector-only RAG\n",
    "    print(\"\\nRunning vector-only RAG...\")\n",
    "    vector_result = vector_only_rag(query, vector_store, k)\n",
    "    \n",
    "    # Run BM25-only RAG\n",
    "    print(\"\\nRunning BM25-only RAG...\")\n",
    "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
    "    \n",
    "    # Run fusion RAG\n",
    "    print(\"\\nRunning fusion RAG...\")\n",
    "    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n",
    "    \n",
    "    # Compare responses from different retrieval methods\n",
    "    print(\"\\nComparing responses...\")\n",
    "    comparison = evaluate_responses(\n",
    "        query, \n",
    "        vector_result[\"response\"], \n",
    "        bm25_result[\"response\"], \n",
    "        fusion_result[\"response\"],\n",
    "        reference_answer\n",
    "    )\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"vector_result\": vector_result,\n",
    "        \"bm25_result\": bm25_result,\n",
    "        \"fusion_result\": fusion_result,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the responses from different retrieval methods.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_response (str): Response from vector-only RAG\n",
    "        bm25_response (str): Response from BM25-only RAG\n",
    "        fusion_response (str): Response from fusion RAG\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation of responses\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
    "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
    "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
    "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
    "\n",
    "    Evaluate the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Comprehensiveness\n",
    "    - Clarity and coherence\"\"\"\n",
    "\n",
    "    # User prompt containing the query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Vector-based response:\n",
    "    {vector_response}\n",
    "\n",
    "    BM25 keyword response:\n",
    "    {bm25_response}\n",
    "\n",
    "    Fusion response:\n",
    "    {fusion_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # Add reference answer to the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "            Reference answer:\n",
    "            {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add instructions for detailed comparison to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
    "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated evaluation content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate fusion retrieval compared to other methods.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
    "    \n",
    "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
    "    chunks, vector_store, bm25_index = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each test query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get the reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Compare retrieval methods for the current query\n",
    "        comparison = compare_retrieval_methods(\n",
    "            query, \n",
    "            chunks, \n",
    "            vector_store, \n",
    "            bm25_index, \n",
    "            k=k, \n",
    "            alpha=alpha,\n",
    "            reference_answer=reference\n",
    "        )\n",
    "        \n",
    "        # Append the comparison results to the results list\n",
    "        results.append(comparison)\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response ===\")\n",
    "        print(comparison[\"vector_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== BM25 Response ===\")\n",
    "        print(comparison[\"bm25_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Fusion Response ===\")\n",
    "        print(comparison[\"fusion_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(comparison[\"comparison\"])\n",
    "    \n",
    "    # Generate an overall analysis of the fusion retrieval performance\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    # Return the results and overall analysis\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluating queries\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems. \n",
    "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
    "    1. Vector-based retrieval (semantic similarity)\n",
    "    2. BM25 keyword retrieval (keyword matching)\n",
    "    3. Fusion retrieval (combination of both)\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where each approach performs best\n",
    "    2. Overall strengths and weaknesses of each approach\n",
    "    3. How fusion retrieval balances the trade-offs\n",
    "    4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries, \n",
    "    provide an overall analysis comparing these three approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n",
    "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
    "\n",
    "    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    # Return the generated analysis content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Fusion Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING FUSION RETRIEVAL ===\n",
      "\n",
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/attention_is_all_you_need.pdf...\n",
      "Created 50 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 50 items to vector store\n",
      "Created BM25 index with 50 documents\n",
      "\n",
      "\n",
      "=== Evaluating Query 1/1 ===\n",
      "Query: What are the main applications of transformer models in natural language processing?\n",
      "\n",
      "=== Comparing retrieval methods for query: What are the main applications of transformer models in natural language processing? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What are the main applications of transformer models in natural language processing?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The provided context primarily discusses the Transformer model's applications in machine translation and English constituency parsing. Specifically, it highlights the model's superior performance in translating between English and German, as well as English and French, achieving state-of-the-art BLEU scores. Additionally, it mentions that the Transformer generalizes well to other tasks, such as English constituency parsing, which involves handling structurally constrained outputs that can be significantly longer than the inputs. \n",
      "\n",
      "Overall, the main applications of Transformer models in natural language processing, as indicated in the context, are:\n",
      "\n",
      "1. Machine Translation (e.g., English-to-German and English-to-French).\n",
      "2. English Constituency Parsing. \n",
      "\n",
      "Other potential applications are not explicitly detailed in the provided context.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The main applications of transformer models in natural language processing, as indicated in the provided context, include:\n",
      "\n",
      "1. **Machine Translation**: The transformer model has been shown to outperform previous models in translation tasks, such as the WMT 2014 English-to-German and English-to-French translation tasks, achieving state-of-the-art BLEU scores.\n",
      "\n",
      "2. **English Constituency Parsing**: The transformer has been evaluated for its ability to generalize to tasks like English constituency parsing, which involves understanding and generating structured outputs.\n",
      "\n",
      "3. **Sequence Modeling and Transduction Problems**: The transformer architecture is positioned as a state-of-the-art approach for various sequence modeling tasks, leveraging self-attention mechanisms to model dependencies within sequences.\n",
      "\n",
      "4. **Reading Comprehension and Abstractive Summarization**: Self-attention has been successfully applied in tasks such as reading comprehension and summarization, indicating the versatility of transformers in handling different types of language understanding tasks.\n",
      "\n",
      "Overall, transformer models are utilized for a range of applications that require understanding and generating human language, particularly in tasks that involve sequence data.\n",
      "\n",
      "=== Fusion Response ===\n",
      "The main applications of transformer models in natural language processing, as indicated in the provided context, include:\n",
      "\n",
      "1. **Machine Translation**: The transformer model has been shown to outperform previous models in tasks such as English-to-German and English-to-French translation, achieving state-of-the-art BLEU scores.\n",
      "\n",
      "2. **English Constituency Parsing**: The transformer can generalize to this task, which involves understanding the structural constraints of language and producing longer outputs than inputs.\n",
      "\n",
      "3. **Reading Comprehension**: Self-attention mechanisms in transformers have been successfully applied to tasks that require understanding and interpreting text.\n",
      "\n",
      "4. **Abstractive Summarization**: Transformers have been used in models designed for generating summaries of texts.\n",
      "\n",
      "5. **Textual Entailment**: The model has been applied to tasks that involve determining the relationship between different pieces of text.\n",
      "\n",
      "6. **Learning Task-Independent Sentence Representations**: Transformers can create representations of sentences that are useful across various tasks.\n",
      "\n",
      "These applications highlight the versatility and effectiveness of transformer models in handling diverse natural language processing challenges.\n",
      "\n",
      "=== Comparison ===\n",
      "### Comparison of Responses\n",
      "\n",
      "#### 1. Vector-based Response\n",
      "- **Relevance to the Query**: The response is relevant but limited. It focuses primarily on machine translation and English constituency parsing, which are indeed significant applications of transformer models. However, it fails to mention other important applications.\n",
      "- **Factual Correctness**: The information provided is factually correct regarding the applications mentioned, particularly the performance in machine translation.\n",
      "- **Comprehensiveness**: The response lacks comprehensiveness, as it only lists two applications and does not explore the broader range of transformer applications in NLP.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, presenting the information in a structured manner. However, the limited scope detracts from its overall effectiveness.\n",
      "\n",
      "#### 2. BM25 Keyword Response\n",
      "- **Relevance to the Query**: This response is highly relevant, covering multiple applications of transformer models in NLP.\n",
      "- **Factual Correctness**: The information is accurate, including specific tasks like machine translation and English constituency parsing, and it introduces additional applications such as sequence modeling, reading comprehension, and abstractive summarization.\n",
      "- **Comprehensiveness**: This response is the most comprehensive of the three, listing four distinct applications and providing a brief explanation of each. It captures a wider range of transformer capabilities.\n",
      "- **Clarity and Coherence**: The response is well-structured and easy to follow, with clear headings for each application. The use of bullet points enhances readability.\n",
      "\n",
      "#### 3. Fusion Response\n",
      "- **Relevance to the Query**: The fusion response is also highly relevant, covering a broad spectrum of applications similar to the BM25 response.\n",
      "- **Factual Correctness**: The information is accurate and aligns well with known applications of transformer models.\n",
      "- **Comprehensiveness**: This response is very comprehensive, listing six applications, including some that were not mentioned in the other responses, such as textual entailment and task-independent sentence representations. This breadth provides a more complete picture of transformer applications.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, with a logical flow and well-defined sections for each application. The use of bullet points aids in clarity.\n",
      "\n",
      "### Overall Evaluation\n",
      "- **Best Approach**: The **Fusion Response** performed the best for this query. It not only included the key applications mentioned in the other responses but also expanded on them significantly, providing a more holistic view of the capabilities of transformer models in NLP.\n",
      "- **Strengths and Weaknesses**:\n",
      "  - **Vector-based Response**: Strength in clarity but weak in comprehensiveness and relevance due to its limited scope.\n",
      "  - **BM25 Keyword Response**: Strong in relevance and comprehensiveness, but slightly less exhaustive than the fusion response.\n",
      "  - **Fusion Response**: Best overall due to its comprehensive coverage and clarity, effectively addressing the query with a wide range of applications.\n",
      "\n",
      "In conclusion, while all three approaches provided relevant information, the fusion retrieval method excelled by combining the strengths of both vector and keyword approaches, resulting in a more informative and complete response.\n",
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "## Overall Analysis of Retrieval Approaches\n",
      "\n",
      "### Overview of Retrieval Techniques\n",
      "In the field of information retrieval, three prominent approaches are commonly used: vector-based retrieval, BM25 keyword retrieval, and fusion retrieval. Each approach has its unique strengths and weaknesses, particularly in handling diverse query types and user needs.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Vector-based Retrieval (Semantic Similarity)\n",
      "**Strengths:**\n",
      "- **Context Understanding**: Vector-based retrieval leverages embeddings from models like BERT or Word2Vec, which can capture context and semantic relationships. This allows for a more nuanced understanding of phrases and synonyms.\n",
      "- **Retrieving Related Concepts**: It is effective for queries that seek comprehensive information on topics or explore relationships between concepts.\n",
      "\n",
      "**Weaknesses:**\n",
      "- **Limited Recall**: May miss more specific or straightforward keyword-based documents, particularly when an exact keyword match is crucial.\n",
      "- **Dependency on Quality of Embeddings**: The results depend heavily on how well the embeddings capture the semantics relevant to the query.\n",
      "\n",
      "**Best Use Cases:**\n",
      "- **Open-Ended Queries**: Ideal for queries looking for related themes (e.g., “applications of transformer models”), allowing for diverse responses.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. BM25 Keyword Retrieval (Keyword Matching)\n",
      "**Strengths:**\n",
      "- **Precision for Specific Queries**: BM25 excels at retrieving documents that contain specific keywords, making it effective for fact-based questions or straightforward queries.\n",
      "- **Clear Relevance Scoring**: The probabilistic model scores documents based on term frequency and inverse document frequency, ensuring high relevancy for exact matches.\n",
      "\n",
      "**Weaknesses:**\n",
      "- **Narrower Contextual Understanding**: Lacks the ability to infer meanings beyond the words provided, potentially leading to missed documents that may use different terms to describe similar concepts.\n",
      "- **Overreliance on Keyword Presence**: It can return a lot of irrelevant documents if the keywords are common, as it doesn’t understand the query intention.\n",
      "\n",
      "**Best Use Cases:**\n",
      "- **Specific Questions**: Best for clear and narrow queries that are looking for facts or defined solutions without much context (e.g., “What is the main application of transformer models?”).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Fusion Retrieval (Combination of Both)\n",
      "**Strengths:**\n",
      "- **Balanced Performance**: Fusion approaches combine strengths of both vector and BM25 methods, often improving the quality of results across diverse query types.\n",
      "- **Improved Recall and Precision**: By integrating both semantic understanding and strict keyword matching, fusion can yield results that are comprehensive yet relevant.\n",
      "\n",
      "**Weaknesses:**\n",
      "- **Complexity**: Implementation may be more complex as it needs effective mechanisms for combining scores (e.g., using weights).\n",
      "- **Overfitting Risk**: There’s a possibility of tuning the system to bias towards one method over the other if not properly balanced.\n",
      "\n",
      "**Best Use Cases:**\n",
      "- **Mixed Queries**: Excellent for varied queries where users seek both broad contextual understanding and specific information. Suitable for research-related inquiries or topics where diverse perspectives are valuable.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Recommendations for Use\n",
      "- **Vector-based Retrieval**: Use this approach when queries are broad, exploratory, and context-sensitive. It’s excellent when dealing with topics that require understanding of relationships between concepts (e.g., “impact of transformer models on NLP tasks”).\n",
      "  \n",
      "- **BM25 Retrieval**: Utilize when there is a clear, specific information need or direct answers required. It is optimal for targeted factual inquiries (e.g., “What is a transformer model?”).\n",
      "  \n",
      "- **Fusion Retrieval**: Opt for this when you anticipate queries that span both exact keywords and require a deeper understanding of various related concepts. It’s particularly advantageous in environments like academic search engines and customer support systems, where a mix of information is often sought.\n",
      "\n",
      "---\n",
      "\n",
      "### Conclusion\n",
      "Evaluating the performance of these approaches reveals that no single method is superior across all scenarios. The choice between vector-based, BM25, or fusion retrieval should be dictated by the nature of the query, the required depth of response, and the potential user intent. Fusion retrieval stands out as an effective strategy to balance the strengths and weaknesses of the other two methods, making it a valuable option in many retrieval scenarios.\n"
     ]
    }
   ],
   "source": [
    "# Path to PDF document\n",
    "# Path to PDF document containing AI information for knowledge retrieval testing\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Define a single AI-related test query\n",
    "test_queries = [\n",
    "    \"What are the main applications of transformer models in natural language processing?\"  # AI-specific query\n",
    "]\n",
    "\n",
    "# Optional reference answer\n",
    "reference_answers = [\n",
    "    \"Transformer models have revolutionized natural language processing with applications including machine translation, text summarization, question answering, sentiment analysis, and text generation. They excel at capturing long-range dependencies in text and have become the foundation for models like BERT, GPT, and T5.\",\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "k = 5  # Number of documents to retrieve\n",
    "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_fusion_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING FUSION RETRIEVAL ===\n",
      "\n",
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf...\n",
      "Created 64 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 64 items to vector store\n",
      "Created BM25 index with 64 documents\n",
      "\n",
      "\n",
      "=== Evaluating Query 1/1 ===\n",
      "Query: What are the strategies to prevent homelessness?\n",
      "\n",
      "=== Comparing retrieval methods for query: What are the strategies to prevent homelessness? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: What are the strategies to prevent homelessness?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The strategies to prevent homelessness, as outlined in the provided context, include the following aims:\n",
      "\n",
      "1. **Prevention of homelessness**: Implementing measures to stop individuals from becoming homeless in the first place.\n",
      "2. **Tackling the causes of homelessness**: Addressing the underlying issues that lead to homelessness.\n",
      "3. **Reducing the level of homelessness**: Actively working to decrease the overall number of homeless individuals.\n",
      "4. **Reducing the negative effects on homeless people and their families**: Mitigating the impact of homelessness on those affected.\n",
      "5. **Ensuring that formerly homeless people can sustain permanent independent housing**: Providing support to help individuals maintain stable housing after experiencing homelessness.\n",
      "\n",
      "To effectively implement these strategies, it is essential to gather accurate information about the homeless population, including those living in temporary or inadequate housing, those at risk of eviction, and those transitioning from institutions without a home.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The strategies to prevent homelessness, as outlined in the provided context, include the following objectives:\n",
      "\n",
      "1. **Prevention of Homelessness**: Policies should aim to ensure that fewer people become homeless by monitoring the total number of homeless households and those at risk of homelessness.\n",
      "\n",
      "2. **Tackling the Causes of Homelessness**: Addressing the underlying factors that contribute to homelessness is essential.\n",
      "\n",
      "3. **Reducing the Level of Homelessness**: Implementing measures to decrease the overall number of homeless individuals.\n",
      "\n",
      "4. **Reducing Negative Effects on Homeless People and Their Families**: Strategies should focus on mitigating the adverse impacts of homelessness on individuals and their families.\n",
      "\n",
      "5. **Ensuring Sustainable Permanent Accommodation**: Providing access to stable and permanent housing for formerly homeless individuals is crucial for preventing recurrence of homelessness.\n",
      "\n",
      "6. **Monitoring Vulnerability to Eviction**: Gathering information on individuals who are at risk of eviction and those leaving institutions without a home is necessary for effective prevention.\n",
      "\n",
      "7. **Visibility of Hidden Homelessness**: Policies should recognize and address the situation of individuals living in insecure or inadequate housing, as well as those frequently moving between unstable housing situations.\n",
      "\n",
      "These strategies require comprehensive data collection and analysis to effectively implement and measure their impact on homelessness.\n",
      "\n",
      "=== Fusion Response ===\n",
      "The strategies to prevent homelessness, as outlined in the provided context, include the following aims:\n",
      "\n",
      "1. **Prevention of Homelessness**: Implementing measures to ensure that fewer people become homeless.\n",
      "2. **Tackling the Causes of Homelessness**: Addressing the underlying issues that lead to homelessness.\n",
      "3. **Reducing the Level of Homelessness**: Actively working to lower the overall number of homeless individuals.\n",
      "4. **Reducing the Negative Effects on Homeless People and Their Families**: Mitigating the impact of homelessness on those affected.\n",
      "5. **Ensuring that Formerly Homeless People Can Sustain Permanent Independent Housing**: Providing support to help previously homeless individuals maintain stable housing.\n",
      "\n",
      "Additionally, the context emphasizes the importance of gathering accurate information to monitor the number of homeless households, those living in temporary or inadequate housing, and individuals vulnerable to eviction. It also highlights the need for sustainable permanent accommodation for formerly homeless people and targeted research to understand hidden homelessness and the circumstances of individuals being discharged from institutions without a home.\n",
      "\n",
      "=== Comparison ===\n",
      "### Comparison of Responses\n",
      "\n",
      "#### 1. Vector-based Retrieval Response\n",
      "\n",
      "**Strengths:**\n",
      "- **Relevance to the Query:** The response is relevant and directly addresses the query about strategies to prevent homelessness.\n",
      "- **Clarity and Coherence:** The structure is clear, with numbered points that make it easy to follow.\n",
      "- **Factual Correctness:** The points made are factually correct and align with common strategies for preventing homelessness.\n",
      "\n",
      "**Weaknesses:**\n",
      "- **Comprehensiveness:** While it covers several key strategies, it lacks depth in explaining how these strategies can be implemented or the importance of data collection.\n",
      "- **Detail:** The response could benefit from more specific examples or case studies to illustrate the strategies.\n",
      "\n",
      "#### 2. BM25 Keyword Retrieval Response\n",
      "\n",
      "**Strengths:**\n",
      "- **Relevance to the Query:** The response is highly relevant and provides a comprehensive list of strategies.\n",
      "- **Comprehensiveness:** It includes additional points such as \"Monitoring Vulnerability to Eviction\" and \"Visibility of Hidden Homelessness,\" which add depth to the response.\n",
      "- **Clarity and Coherence:** The response is well-structured and easy to read, with clear headings for each strategy.\n",
      "\n",
      "**Weaknesses:**\n",
      "- **Factual Correctness:** While the strategies are correct, the response could be seen as slightly repetitive in phrasing compared to the vector-based response.\n",
      "- **Lack of Specificity:** Similar to the vector-based response, it lacks specific examples or actionable steps for implementation.\n",
      "\n",
      "#### 3. Fusion Retrieval Response\n",
      "\n",
      "**Strengths:**\n",
      "- **Relevance to the Query:** The response is relevant and covers the main strategies effectively.\n",
      "- **Comprehensiveness:** It combines elements from both the vector and BM25 responses, including the importance of data collection and addressing hidden homelessness.\n",
      "- **Clarity and Coherence:** The response is well-organized and flows logically, making it easy to understand.\n",
      "\n",
      "**Weaknesses:**\n",
      "- **Repetition:** Some points are similar to those in the other responses, which may not add new value.\n",
      "- **Detail:** While it mentions the importance of data collection, it could provide more specific examples or methods for how to gather this data.\n",
      "\n",
      "### Overall Evaluation\n",
      "\n",
      "**Best Approach: Fusion Retrieval**\n",
      "- The fusion retrieval approach performed the best for this query. It effectively combines the strengths of both vector-based and BM25 keyword retrieval methods, providing a comprehensive and relevant response that addresses the query in detail. It includes additional insights about data collection and the visibility of hidden homelessness, which are crucial for understanding the broader context of homelessness prevention.\n",
      "\n",
      "### Summary of Strengths and Weaknesses\n",
      "\n",
      "- **Vector-based Retrieval:**\n",
      "  - **Strengths:** Clear structure, relevant, factually correct.\n",
      "  - **Weaknesses:** Lacks depth and specific examples.\n",
      "\n",
      "- **BM25 Keyword Retrieval:**\n",
      "  - **Strengths:** Comprehensive, includes additional relevant points, clear structure.\n",
      "  - **Weaknesses:** Slightly repetitive, lacks specificity in implementation.\n",
      "\n",
      "- **Fusion Retrieval:**\n",
      "  - **Strengths:** Combines strengths of both methods, comprehensive, addresses data collection and hidden homelessness.\n",
      "  - **Weaknesses:** Some repetition, could benefit from more specific examples.\n",
      "\n",
      "In conclusion, while all three approaches provided relevant information, the fusion retrieval approach stood out for its comprehensiveness and ability to synthesize information effectively, making it the most suitable response for the query on strategies to prevent homelessness.\n",
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "### Analysis of Vector-based, BM25, and Fusion Retrieval Approaches\n",
      "\n",
      "**1. Types of Queries Where Each Approach Performs Best**\n",
      "\n",
      "- **Vector-based Retrieval (Semantic Similarity):**\n",
      "  - **Best for:** Conceptual queries or those requiring understanding of context and intention, such as queries that are broad or ambiguous. This approach excels in finding relevant documents even if they do not contain the exact keywords from the query.\n",
      "  - **Example:** “Strategies to prevent homelessness” - A vector-based approach can identify documents discussing related concepts like “housing stability” or “social programs”.\n",
      "\n",
      "- **BM25 Keyword Retrieval (Keyword Matching):**\n",
      "  - **Best for:** Precise queries where the user is likely looking for specific information or documents that contain exact phrases. This model thrives in cases where the query is clear and direct.\n",
      "  - **Example:** In a query that specifies “laws against homelessness,” BM25 would perform well as it precisely matches keywords in a database.\n",
      "\n",
      "- **Fusion Retrieval (Combination of Both):**\n",
      "  - **Best for:** Queries that fall in between and require both conceptual understanding and precise keyword matching. Fusion retrieval is useful in scenarios where the user may not know the exact terminology or where documents vary in lexical style.\n",
      "  - **Example:** Complex queries like “Social innovative programs that can mitigate homelessness issues” will benefit from both semantic understanding and keyword specificity. \n",
      "\n",
      "**2. Overall Strengths and Weaknesses of Each Approach**\n",
      "\n",
      "- **Vector-based Retrieval:**\n",
      "  - **Strengths:**\n",
      "    - Captures semantic relationships between words, leading to a broader relevance.\n",
      "    - Handles synonyms and variations well due to its vector space representation.\n",
      "  - **Weaknesses:**\n",
      "    - May retrieve documents that are relevant in concept but not more targeted, which may not necessarily match user expectations.\n",
      "    - Requires effective preprocessing and often complex models, which can lead to higher computational costs.\n",
      "\n",
      "- **BM25 Keyword Retrieval:**\n",
      "  - **Strengths:**\n",
      "    - Highly efficient and fast due to simplicity in ranking documents based on term frequency and inverse document frequency.\n",
      "    - Provides consistent and predictable results for clear keyword matches.\n",
      "  - **Weaknesses:**\n",
      "    - Struggles with synonyms, paraphrasing, and less common phrasing.\n",
      "    - Often ignores context and deeper meaning behind queries, which can lead to lower satisfaction for nuanced queries.\n",
      "\n",
      "- **Fusion Retrieval:**\n",
      "  - **Strengths:**\n",
      "    - Leverages the strengths of both vector-based and BM25 approaches, leading to a holistic retrieval process.\n",
      "    - Can adapt to various types of queries and user intents, serving a wider audience effectively.\n",
      "  - **Weaknesses:**\n",
      "    - May introduce complexity in implementation and requires optimization to balance contributions from each method.\n",
      "    - Responses could be diluted if one method significantly overshadows the other or if the balance isn't tuned properly.\n",
      "\n",
      "**3. How Fusion Retrieval Balances the Trade-offs**\n",
      "\n",
      "Fusion retrieval combines the strengths of both vector-based and BM25 methods, allowing it to adapt to a diverse range of queries. By adjusting the weighting of each component (semantic relevance and keyword matching), it can cater to user intention more effectively. For instance, in a query about strategies to prevent homelessness, fusion could prioritize documents with relevant keywords while also pulling in conceptually related literature. It effectively reduces the shortfalls of both individual methods — leveraging semantic understanding to enhance keyword results while also retaining the precision offered by strong keyword matches.\n",
      "\n",
      "**4. Recommendations for When to Use Each Approach**\n",
      "\n",
      "- **Use Vector-based Retrieval:**\n",
      "  - When the query lacks specific terminology or has high variability in language, particularly in fields like social sciences, humanities, or emerging topics where terms are still being defined.\n",
      " \n",
      "- **Use BM25 Keyword Retrieval:**\n",
      "  - When speed and keyword precision are paramount, especially in well-defined domains like legal documents, technical manuals, or FAQs where terms are consistent and specific.\n",
      "\n",
      "- **Use Fusion Retrieval:**\n",
      "  - When dealing with diverse queries that may embody elements of both conceptual and precise information needs. It is especially recommended for comprehensive systems where a wide array of queries is expected, such as in search engines, academic databases, or customer service repositories.\n",
      "\n",
      "In conclusion, the choice of retrieval approach depends on the specifics of the queries and the information landscape. Fusion retrieval often provides an optimal solution in scenarios where users experience a broad spectrum of informational needs, balancing flexibility with precision.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to PDF document\n",
    "# Path to PDF document containing AI information for knowledge retrieval testing\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# Define a single AI-related test query\n",
    "test_queries = [\n",
    "    \"What are the strategies to prevent homelessness?\"  # AI-specific query\n",
    "]\n",
    "\n",
    "# Optional reference answer\n",
    "reference_answers = [\n",
    "   \n",
    "\n",
    "    \"Prevent new incidences of homelessness through early intervention and support services.\",  \"Address and mitigate the underlying causes of homelessness, such as poverty, unemployment, and lack of affordable housing.\", \"Reduce the overall number of people experiencing homelessness via rapid rehousing and housing-first models.\", \"Minimize the negative social, health, and economic impacts on individuals and families currently experiencing homelessness.\", \"Ensure formerly homeless people maintain permanent, independent housing through ongoing support and follow-up services.\"\n",
    "    \n",
    "    ]\n",
    "\n",
    "# Set parameters\n",
    "k = 5  # Number of documents to retrieve\n",
    "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_fusion_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISEM 770 Class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
